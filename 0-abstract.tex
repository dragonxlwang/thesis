\begin{abstract}
In this thesis, we conduct a study of probabilistic latent vairable models with
applications to knowledge discovery and optimizations. Probabilistic modeling is
one of the most significant principled means to gain insight of the data. By
assuming the observations of the data are generated from a distribution, we can
estimate its density, or the statistics of our interest, by either Maximum
Likelihood or Bayesian inference, depending whether there is a prior
distribution for the parameters of assumed data distribution.

It is more important to us to reveal the underlying knowledge of the data, and
the common practice is to model the latent variables in addition to the
observations. Such latent variables naturally compute, for example, the classes
(labels), the clusters, and other unobserved measurements of the data. Besides,
through introducing the latent variables, it also facilities the optimization,
and provides a trade-off between performance and efficiency.

We describe a range of applications where latent variables can be leveraged to
explore the knowledge of the data and speed up the optimization. Our results
lead to efficient algorithms which take the advantages of latent variables in
probabilistic models. We conduct experiments against state-of-the-art approaches
and the empirical evaluation shows that probabilistic latent variable models
improve the learning performance as well as computation efficiency.

\end{abstract}
