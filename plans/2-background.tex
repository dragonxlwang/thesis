\section{Background}

This chapter addresses the problem of jointly learning phrasal allocation and phrase
embedding. To our best knowledge, this is the first work that integrates
the two tasks into one framework. To give a fair account of related work, we
discuss previous studies on each task.

\subsection{Phrasal Allocation}

It was a classic problem to extract phrases from unstructured text. Approaches
by analyzing the co-occurrence
frequencies~\cite{witten1999kea,lindsey2012phrase,wang2007topical}, or
information-theoretic measurements such as pointwise mutual
information~(PMI)~\cite{fano1961transmission,church1990word} and generalized
mutual information~(GMI)~\cite{magerman1990parsing} are proposed. One common
drawback shared by methods of this line is that it is difficult to compare the
importance of phrases containing variable length of words. One implication is
that they are hardly scalable when learning from large corpora in the online
stream setting as it is unrealistic to keep track of all n-grams. Another aspect
of limitation is that though it is hard to find segmentation of phrases with
respect of the context. For example, in ``New York Times Square'', ``New York
Times'' would likely to be recognized as a phrase even though a better
segmentation should be ``New York'' and ``Times square''.

\subsection{Embedding Learning}

It was recently brought to people's attention that distributed representation of
words can be leveraged for NLP learning tasks such as text classification. Among
different word embedding algorithms, ``Skip-gram''~(SG) and ``Continuous
bag-of-words''~(CBOW) are simple yet effective. The underlying idea is to model
the relationship between words co-occurred in a short window. Take SG as an
example, it computes the probability of seeing a context word given the target
(center) word and optimizes the word embedding via Negative Sampling, which is a
simplified version of the Noise-Contrastive
Estimation~(NCE)~\cite{gutmann2010noise}.  Word embedding trained on large
corpora shows good property by preserving the semantic relationship between
words, for instance, $\text{vec}(``woman") - \text{vec}(``man") \approx
\text{vec}(``king") - \text{vec}(``queen")$.

Beyond word embedding, compositional methods are investigated, where the
embedding of a higher language unit such as phrase (or sentence): $v_{A B}$ is
computed by composition function $f(v_A, v_B)$. Since the input and the output
are vectors of the same dimension, recursive neural networks are used to learn the
composition, as in \cite{le2015compositional,irsoy2014deep,socher2013parsing}.
However, they all require additional parsing tree for training and can only
accommodate binary composition. Another composition, using convolution neural
network, is also leveraged to model sentence embedding for classification, which
achieves superior performance than simple word embedding averaging. Though
compositional methods are more flexible than word embedding, they lacks the
capability to model phrases whose semantics are not decomposable to its
constituent words, such as ``White House'' and ``New York Times''. To address
this, \cite{yin2014exploration} proposed a preprocessing strategy to perform
phrase segmentation by a dictionary of phrases. Phrase embedding is learnt for
each multi-term phrase and single-term word. Although all aforementioned works
have enjoyed improved performance by modeling phrase or sentence embeddings,
they did not include the phrasal allocation into their learning task.

